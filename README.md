# Awesome-Human-Motion-Generation

A list of awesome human motion generation papers. Continuing to be updated!!!

## Papers

> ⭐️ indicates that the paper introduces a new dataset.

### Action-to-Motion

- [MultiAct: Long-Term 3D Human Motion Generation from Multiple Action Labels](https://arxiv.org/pdf/2212.05897), AAAI 2023 | [Code](https://github.com/TaeryungLee/MultiAct_RELEASE)

- [Action-conditioned On-demand Motion Generation](https://arxiv.org/pdf/2207.08164), MM 2022 | [Project](https://roychowdhuryresearch.github.io/ODMO_ACMMM2022/) | [Code](https://github.com/roychowdhuryresearch/ODMO)

- [Action-Conditioned 3D Human Motion Synthesis with Transformer VAE](https://arxiv.org/pdf/2104.05670), ICCV 2021 | [Project](https://mathis.petrovich.fr/actor/) | [Code](https://github.com/Mathux/ACTOR)

- [BABEL: Bodies, Action and Behavior with English Labels](https://arxiv.org/pdf/2106.09696), CVPR 2021 | [Project](https://babel.is.tue.mpg.de/) | [Code](https://github.com/abhinanda-punnakkal/BABEL)
⭐️

- [Action2Motion: Conditioned Generation of 3D Human Motions](https://arxiv.org/pdf/2007.15240), MM 2020 | [Project](https://ericguo5513.github.io/action-to-motion/) | [Code](https://github.com/EricGuo5513/action-to-motion)



### Text-to-Motion

- [MotionLCM: Real-time Controllable Motion Generation via Latent Consistency Model](https://arxiv.org/abs/2404.19759), ECCV 2024 | [Project](https://dai-wenxun.github.io/MotionLCM-page/) | [Code](https://github.com/Dai-Wenxun/MotionLCM)

- [AvatarGPT: All-in-One Framework for Motion Understanding, Planning, Generation and Beyond](https://arxiv.org/pdf/2311.16468), CVPR 2024 | [Project](https://zixiangzhou916.github.io/AvatarGPT/) | [Code](https://github.com/zixiangzhou916/AvatarGPT)

- [MoMask: Generative Masked Modeling of 3D Human Motions](https://arxiv.org/pdf/2312.00063), CVPR 2024 | [Project](https://ericguo5513.github.io/momask/) | [Code](https://github.com/EricGuo5513/momask-codes)
  
- [HumanTOMATO: Text-aligned Whole-body Motion Generation](https://arxiv.org/pdf/2310.12978), ICML 2024 | [Project](https://lhchen.top/HumanTOMATO) | [Code](https://github.com/IDEA-Research/HumanTOMATO)

- [MotionGPT: Human Motion as Foreign Language](https://arxiv.org/abs/2306.14795), NeurIPS 2023 | [Project](https://motion-gpt.github.io/) | [Code](https://github.com/OpenMotionLab/MotionGPT)

- [FineMoGen: Fine-Grained Spatio-Temporal Motion Generation and Editing](https://arxiv.org/pdf/2312.15004), NeurIPS 2023 | [Project](https://mingyuan-zhang.github.io/projects/FineMoGen.html) | [Code](https://github.com/mingyuan-zhang/FineMoGen)

- [Motion-X: A Large-scale 3D Expressive Whole-body Human Motion Dataset](https://arxiv.org/pdf/2307.00818), NeurIPS 2023 | [Project](https://motion-x-dataset.github.io/) | [Code](https://github.com/IDEA-Research/Motion-X)
⭐️

- [ReMoDiffuse: Retrieval-Augmented Motion Diffusion Model](https://arxiv.org/abs/2304.01116), ICCV 2023 | [Project](https://mingyuan-zhang.github.io/projects/ReMoDiffuse.html) | [Code](https://github.com/mingyuan-zhang/ReMoDiffuse)

- [UDE: A Unified Driving Engine for Human Motion Generation](https://arxiv.org/pdf/2211.16016), CVPR 2023 | [Project](https://zixiangzhou916.github.io/UDE/) | [Code](https://github.com/zixiangzhou916/UDE)

- [T2M-GPT: Generating Human Motion from Textual Descriptions with Discrete Representations](https://arxiv.org/abs/2301.06052), CVPR 2023 | [Project](https://mael-zys.github.io/T2M-GPT/) | [Code](https://github.com/Mael-zys/T2M-GPT)

- [Executing your Commands via Motion Diffusion in Latent Space](https://arxiv.org/pdf/2212.04048), CVPR 2023 | [Project](https://chenxin.tech/mld) | [Code](https://github.com/ChenFengYe/motion-latent-diffusion)

- [SINC: Spatial Composition of 3D Human Motions for Simultaneous Action Generation](https://arxiv.org/pdf/2304.10417), ICCV 2023 | [Project](https://sinc.is.tue.mpg.de/) | [Code](https://github.com/athn-nik/sinc)

- [FLAME: Free-form Language-based Motion Synthesis & Editing](https://arxiv.org/pdf/2209.00349), AAAI 2023 | [Project](https://kakaobrain.github.io/flame/) | [Code](https://github.com/kakaobrain/flame)

- [Human Motion Diffusion Model](https://arxiv.org/pdf/2209.14916), ICLR 2023 | [Project](https://guytevet.github.io/mdm-page/) | [Code](https://github.com/GuyTevet/motion-diffusion-model)

- [MotionDiffuse: Text-Driven Human Motion Generation with Diffusion Model](https://arxiv.org/pdf/2208.15001), ArXiv 2022 | [Project](https://mingyuan-zhang.github.io/projects/MotionDiffuse.html) | [Code](https://github.com/mingyuan-zhang/MotionDiffuse)

- [TEACH: Temporal Action Composition for 3D Humans](https://arxiv.org/pdf/2209.04066), 3DV 2022 | [Project](https://teach.is.tue.mpg.de/) | [Code](https://github.com/athn-nik/teach)

- [TEMOS: Generating Diverse Human Motions from Textual Descriptions](https://arxiv.org/abs/2204.14109), ECCV 2022 | [Project](https://mathis.petrovich.fr/temos/) | [Code](https://github.com/Mathux/TEMOS)

- [TM2T: Stochastic and Tokenized Modeling for the Reciprocal Generation of 3D Human Motions and Texts](https://arxiv.org/pdf/2207.01696), ECCV 2022 | [Project](https://ericguo5513.github.io/TM2T) | [Code](https://github.com/EricGuo5513/TM2T)

- [MotionCLIP: Exposing Human Motion Generation to CLIP Space](https://arxiv.org/pdf/2203.08063), ECCV 2022 | [Project](https://guytevet.github.io/motionclip-page/) | [Code](https://github.com/GuyTevet/MotionCLIP)

- [Generating Diverse and Natural 3D Human Motions from Text](https://openaccess.thecvf.com/content/CVPR2022/papers/Guo_Generating_Diverse_and_Natural_3D_Human_Motions_From_Text_CVPR_2022_paper.pdf), CVPR 2022 | [Project](https://ericguo5513.github.io/text-to-motion/) | [Code](https://github.com/EricGuo5513/HumanML3D)
⭐️

- [AMASS: Archive of Motion Capture As Surface Shapes](https://arxiv.org/pdf/1904.03278), ICCV 2019 | [Project](https://amass.is.tue.mpg.de/) | [Code](https://github.com/nghorbani/amass)
⭐️

- [Synthesis of Compositional Animations from Textual Descriptions](https://arxiv.org/pdf/2103.14675), ICCV 2021 | [Code](https://github.com/anindita127/Complextext2animation)

- [Language2Pose: Natural Language Grounded Pose Forecasting](https://arxiv.org/pdf/1907.01108), 3DV 2019 | [Project](https://chahuja.com/language2pose/) | [Code](https://github.com/chahuja/language2pose)




### Audio-to-Motion

- [Emotional Speech-driven 3D Body Animation via Disentangled Latent Diffusion](https://arxiv.org/pdf/2312.04466), CVPR 2024 | [Project](https://amuse.is.tue.mpg.de/) | [Code](https://github.com/kiranchhatre/amuse)

- [Listen, Denoise, Action! Audio-Driven Motion Synthesis with Diffusion Models](https://arxiv.org/pdf/2211.09707), TOG 2023 | [Project](https://www.speech.kth.se/research/listen-denoise-action/) | [Code](https://github.com/simonalexanderson/ListenDenoiseAction/)
  
- [Music-Driven Group Choreography](https://arxiv.org/pdf/2303.12337), CVPR 2023 | [Project](https://aioz-ai.github.io/AIOZ-GDANCE/) | [Code](https://github.com/aioz-ai/AIOZ-GDANCE)
⭐️

- [EDGE: Editable Dance Generation From Music](https://arxiv.org/pdf/2211.10658), CVPR 2023 | [Project](https://edge-dance.github.io/) | [Code](https://github.com/Stanford-TML/EDGE)

- [Generating Holistic 3D Human Motion from Speech](https://arxiv.org/pdf/2212.04420), CVPR 2023 | [Project](https://talkshow.is.tue.mpg.de/) | [Code](https://github.com/yhw-yhw/SHOW)

- [DanceFormer: Music Conditioned 3D Dance Generation with Parametric Motion Transformer](https://arxiv.org/pdf/2103.10206), AAAI 2022 | [Code](https://github.com/libuyu/PhantomDanceDataset)

- [Audio2Gestures: Generating Diverse Gestures from Speech Audio with Conditional Variational Autoencoders](https://arxiv.org/pdf/2108.06720), ICCV 2021 | [Code](https://github.com/JingLi513/Audio2Gestures)

- [Dancing to Music](https://arxiv.org/pdf/1911.02001), NeurIPS 2019 | [Code](https://github.com/NVlabs/Dancing2Music)




### Object-to-Motion (HOI)

- [CORE4D: A 4D Human-Object-Human Interaction Dataset for Collaborative Object REarrangement](https://arxiv.org/pdf/2406.19353), ArXiv 2024 | [Project](https://core4d.github.io/) | [Code](https://github.com/leolyliu/CORE4D-Instructions)
⭐️

- [Full-Body Articulated Human-Object Interaction](https://arxiv.org/pdf/2212.10621), ICCV 2023 | [Project](https://jnnan.github.io/project/chairs) | [Code](https://github.com/jnnan/chairs)
⭐️

- [IMos: Intent-Driven Full-Body Motion Synthesis for Human-Object Interactions](https://arxiv.org/pdf/2212.07555), Eurographics 2023 | [Project](https://vcai.mpi-inf.mpg.de/projects/IMoS/) | [Code](https://github.com/anindita127/IMoS)
  
- [Object Motion Guided Human Motion Synthesis](https://arxiv.org/pdf/2309.16237), SIGGRAPH Asia 2023 | [Project](https://lijiaman.github.io/projects/omomo/) | [Code](https://github.com/lijiaman/omomo_release)
⭐️

- [SAGA: Stochastic Whole-Body Grasping With Contact](https://arxiv.org/pdf/2112.10103), ECCV 2022 | [Project](https://jiahaoplus.github.io/SAGA/saga.html) | [Code](https://github.com/JiahaoPlus/SAGA)

- [COUCH: Towards Controllable Human-Chair Interactions](http://arxiv.org/pdf/2205.00541), ECCV 2022 | [Project](https://virtualhumans.mpi-inf.mpg.de/couch/) | [Code](https://github.com/xz6014/couch/)
⭐️

- [BEHAVE: Dataset and Method for Tracking Human Object Interactions](https://arxiv.org/pdf/2204.06950), CVPR 2022 | [Project](https://virtualhumans.mpi-inf.mpg.de/behave/)
⭐️

- [GOAL: Generating 4D Whole-Body Motion for Hand-Object Grasping](https://arxiv.org/pdf/2112.11454), CVPR 2022 | [Project](https://goal.is.tue.mpg.de/) | [Code](https://github.com/otaheri/GOAL)

- [Learning to Sit: Synthesizing Human-Chair Interactions via Hierarchical Control](https://arxiv.org/pdf/1908.07423), AAAI 2021 | [Project](https://sites.google.com/view/hierarchical-control)

- [GRAB: A Dataset of Whole-Body Human Grasping of Objects](https://arxiv.org/abs/2008.11200), ECCV 2020 | [Project](https://grab.is.tue.mpg.de/) | [Code](https://github.com/otaheri/GRAB)
⭐️



### Scene-to-Motion (HSI)

- [Scaling Up Dynamic Human-Scene Interaction Modeling](https://arxiv.org/pdf/2403.08629), CVPR 2024 | [Project](https://jnnan.github.io/trumans/) | [Code](https://github.com/jnnan/trumans_utils)
⭐️

- [Synthesizing Diverse Human Motions in 3D Indoor Scenes](https://arxiv.org/pdf/2305.12411), ICCV 2023 | [Project](https://zkf1997.github.io/DIMOS/) | [Code](https://github.com/zkf1997/DIMOS)

- [Narrator: Towards Natural Control of Human-Scene Interaction Generation via Relationship Reasoning](https://arxiv.org/pdf/2303.09410), ICCV 2023 | [Project](https://cic.tju.edu.cn/faculty/likun/projects/Narrator/index.html)

- [CIRCLE: Capture in Rich Contextual Environments](https://arxiv.org/pdf/2303.17912), CVPR 2023 | [Project](https://stanford-tml.github.io/circle_dataset/)
⭐️

- [Generating Continual Human Motion in Diverse 3D Scenes](https://arxiv.org/pdf/2304.02061), 3DV 2023

- [Diffusion-based Generation, Optimization, and Planning in 3D Scenes](https://arxiv.org/pdf/2301.06015), CVPR 2023 | [Project](https://scenediffuser.github.io/) | [Code](https://github.com/scenediffuser/Scene-Diffuser)

- [Towards Diverse and Natural Scene-aware 3D Human Motion Synthesis](https://arxiv.org/pdf/2205.13001), CVPR 2022

- [Scene-aware Generative Network for Human Motion Synthesis](https://arxiv.org/pdf/2105.14804), CVPR 2021

- [Synthesizing Long-Term 3D Human Motion and Interaction in 3D Scenes](https://arxiv.org/pdf/2012.05522), CVPR 2021 | [Project](https://jiashunwang.github.io/Long-term-Motion-in-3D-Scenes/) | [Code](https://github.com/jiashunwang/Long-term-Motion-in-3D-Scenes)

- [Stochastic Scene-Aware Motion Prediction](https://arxiv.org/pdf/2108.08284), ICCV 2021 | [Project](https://samp.is.tue.mpg.de/) | [Code](https://github.com/mohamedhassanmus/SAMP)

- [Long-term Human Motion Prediction with Scene Context](https://arxiv.org/pdf/2007.03672), ECCV 2020 | [Project](https://zhec.github.io/hmp/) | [Code](https://github.com/ZheC/GTA-IM-Dataset)
⭐️



### Multi-Human Generation

- [CrowdMoGen: Zero-Shot Text-Driven Collective Motion Generation](https://arxiv.org/abs/2407.06188), ArXiv 2024 | [Project](https://gxyes.github.io/projects/CrowdMoGen.html) | [Code](https://github.com/gxyes/CrowdMoGen)

- [ReGenNet: Towards Human Action-Reaction Synthesis](https://arxiv.org/pdf/2403.11882), CVPR 2024 | [Project](https://liangxuy.github.io/ReGenNet/) | [Code](https://github.com/liangxuy/ReGenNet)

- [Inter-X: Towards Versatile Human-Human Interaction Analysis](https://arxiv.org/pdf/2312.16051), CVPR 2024 | [Project](https://liangxuy.github.io/inter-x/) | [Code](https://github.com/liangxuy/Inter-X)
⭐️

- [InterGen: Diffusion-based Multi-human Motion Generation under Complex Interactions](https://arxiv.org/pdf/2304.05684), IJCV 2024 | [Project](https://tr3e.github.io/intergen-page/) | [Code](https://github.com/tr3e/InterGen)
⭐️



### Multi-Modal Conditional Motion Generation

- [Generating Human Interaction Motions in Scenes with Text Control](https://arxiv.org/pdf/2404.10685.pdf), ECCV 2024 | [Project](https://research.nvidia.com/labs/toronto-ai/tesmo/) | [Code](https://research.nvidia.com/labs/toronto-ai/tesmo/)

- [Controllable Human-Object Interaction Synthesis](https://arxiv.org/pdf/2312.03913.pdf), ECCV 2024 | [Project](https://lijiaman.github.io/projects/chois/)

- [CG-HOI: Contact-Guided 3D Human-Object Interaction Generation](https://arxiv.org/abs/2311.16097), CVPR 2024 | [Project](https://www.christian-diller.de/projects/cg-hoi/)

- [Generating Human Motion in 3D Scenes from Text Descriptions](https://arxiv.org/pdf/2405.07784), CVPR 2024 | [Project](https://zju3dv.github.io/text_scene_motion/) | [Code](https://github.com/zju3dv/text_scene_motion)

- [Move as You Say, Interact as You Can: Language-guided Human Motion Generation with Scene Affordance](https://arxiv.org/pdf/2403.18036), CVPR 2024 | [Project](https://afford-motion.github.io/) | [Code](https://github.com/afford-motion/afford-motion)

- [HOI-Diff: Text-Driven Synthesis of 3D Human-Object Interactions using Diffusion Models](https://arxiv.org/abs/2312.06553), ArXiv 2023 | [Project](https://neu-vi.github.io/HOI-Diff/) | [Code](https://github.com/neu-vi/HOI-Diff)

- [HUMANISE: Language-conditioned Human Motion Generation in 3D Scenes](https://arxiv.org/pdf/2210.09729), NeurIPS 2022 | [Project](https://silvester.wang/HUMANISE/) | [Code](https://github.com/Silverster98/HUMANISE)
⭐️



### Open-vocabulary & Open-World

- [Plan, Posture and Go: Towards Open-World Text-to-Motion Generation](https://arxiv.org/pdf/2312.14828), CVPR 2024 | [Project](https://moonsliu.github.io/Pro-Motion/)

- [Programmable Motion Generation for Open-Set Motion Control Tasks](https://arxiv.org/abs/2405.19283), CVPR 2024 | [Project](https://hanchaoliu.github.io/Prog-MoGen/) | [Code](https://github.com/HanchaoLiu/ProgMoGen)

- [OMG: Towards Open-vocabulary Motion Generation via Mixture of Controllers](https://arxiv.org/pdf/2312.08985), CVPR 2024 | [Project](https://tr3e.github.io/omg-page/)

- [Being Comes from Not-being: Open-vocabulary Text-to-Motion Generation with Wordless Training](https://arxiv.org/pdf/2210.15929), CVPR 2023



### Physics-based Human Motion

- [CooHOI: Learning Cooperative Human-Object Interaction with Manipulated Object Dynamics](https://arxiv.org/pdf/2406.14558), ArXiv 2024

- [Anyskill: Learning openvocabulary physical skill for interactive agents](https://arxiv.org/pdf/2403.12835), CVPR 2024 | [Project](https://anyskill.github.io/) | [Code](https://github.com/jiemingcui/anyskill)

- [Unified Human-Scene Interaction via Prompted Chain-of-Contacts](https://arxiv.org/pdf/2309.07918), ICLR 2024 | [Project](https://xizaoqu.github.io/unihsi/) | [Code](https://github.com/OpenRobotLab/UniHSI)

- [Synthesizing Physically Plausible Human Motions in 3D Scenes](https://arxiv.org/pdf/2308.09036), 3DV 2024 | [Project](https://idc-flash.github.io/InterScene/) | [Code](https://github.com/liangpan99/InterScene)

- [Synthesizing Physical Character-Scene Interactions](https://arxiv.org/pdf/2302.00883), SIGGRAPH 2023 | [Project](https://xbpeng.github.io/projects/InterPhys/index.html)

- [PhysHOI: Physics-Based Imitation of Dynamic Human-Object Interaction](https://arxiv.org/abs/2312.04393), ArXiv 2023 | [Project](https://wyhuai.github.io/physhoi-page/) | [Code](https://github.com/wyhuai/PhysHOI)

- [InsActor: Instruction-driven Physics-based Characters](https://arxiv.org/abs/2312.17135), NeurIPS 2023 | [Project](https://jiawei-ren.github.io/projects/insactor/index.html) | [Code](https://github.com/jiawei-ren/insactor)

- [ASE: Large-Scale Reusable Adversarial Skill Embeddings for Physically Simulated Characters](https://arxiv.org/pdf/2205.01906), SIGGRAPH 2022 | [Project](https://xbpeng.github.io/projects/ASE/index.html) | [Code](https://github.com/nv-tlabs/ASE/)

- [AMP: Adversarial Motion Priors for Stylized Physics-Based Character Control](https://arxiv.org/pdf/2104.02180), SIGGRAPH 2021 | [Project](https://xbpeng.github.io/projects/AMP/index.html) | [Code](https://github.com/xbpeng/DeepMimic)

- [DeepMimic: Example-Guided Deep Reinforcement Learning of Physics-Based Character Skills](https://arxiv.org/pdf/1804.02717), SIGGRAPH 2018 | [Project](https://xbpeng.github.io/projects/DeepMimic/index.html) | [Code](https://github.com/xbpeng/DeepMimic)




### Others

- [Interactive Character Control with Auto-Regressive Motion Diffusion Models](https://arxiv.org/pdf/2306.00416v2), SIGGRAPH 2024 | [Project](https://xbpeng.github.io/projects/AMDM/index.html) | [Code](https://github.com/Yi-Shi94/AMDM)

- [WANDR: Intention-guided Human Motion Generation](https://arxiv.org/pdf/2404.15383.pdf), CVPR 2024 | [Project](https://wandr.is.tue.mpg.de/) | [Code](https://github.com/markos-diomataris/wandr)

- [MAS: Multi-view Ancestral Sampling for 3D motion generation using 2D diffusion](https://arxiv.org/pdf/2310.14729), CVPR 2024 | [Project](https://guytevet.github.io/mas-page/) | [Code](https://github.com/roykapon/MAS)

- [Optimizing Diffusion Noise Can Serve As Universal Motion Priors](https://arxiv.org/abs/2312.11994), CVPR 2024 | [Project](https://korrawe.github.io/dno-project/) | [Code](https://github.com/korrawe/Diffusion-Noise-Optimization)

- [GMD: Controllable Human Motion Synthesis via Guided Diffusion Models](https://arxiv.org/pdf/2305.12577), ICCV 2023 | [Project](https://korrawe.github.io/gmd-project/) | [Code](https://github.com/korrawe/guided-motion-diffusion)

- [PhysDiff: Physics-Guided Human Motion Diffusion Model](https://arxiv.org/pdf/2212.02500), ICCV 2023 | [Project](https://nvlabs.github.io/PhysDiff)

- [Trace and Pace: Controllable Pedestrian Animation via Guided Trajectory Diffusion](https://arxiv.org/pdf/2304.01893), CVPR 2023 | [Project](https://research.nvidia.com/labs/toronto-ai/trace-pace/) | [Code](https://github.com/nv-tlabs/trace)

## Contributing

Please feel free to pull requests to add papers. I will merge your PR as soon as possible once it is checked.

## License

This repository is distributed under the terms of the MIT license. See [LICENSE](LICENSE) for details.

## Contact

If you have any questions, please feel free to contact me via `zanwang98@gmail.com`.
